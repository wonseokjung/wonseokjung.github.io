---
title: "Prediction with linear approximation"
date: 2018-07-26 12:26:28 -0400
categories: Reinforcementlearning update
use_math : true
---

Value function approximation (VFA)는 Table 형태를 neneral parameterized form으로 바꿔주는 역할을 한다. 

세상의 모든 현상을 파악하려면 엄청나게 많은 계산량 또는 저장량이 필요할 것이다. 

하지만 현대 인류는 한정된 용량의 뇌, 자원, 기계를 사용하고 있기에 세상의 모든 현상을 정확히 알수 없으므로 **예측**을 한다. 


작거나 간단한 Environment에서는 모든 data를 저장하고 Optimal Policy를 찾았던 table case 대신 
 
Value function approximation (VFA)는 Table 형태를 general parameterized form으로 바꿔주는 역할을 한다. 



<img src="https://www.dropbox.com/s/hun87igez9eqfo6/Screenshot%202018-07-11%2018.29.33.png?raw=1">



# 1. State value
State $S_t$가 function approximation box의 input으로 들어가며, function approxmiation의 weight(w)에 의해 state value를 output으로 return한다. 

학습을 할때 목표값이나 정답값이 존재하여야 하며 그것을 Target이라고 한다. 

이 target은 여럭지가 될 수 있다. 사람의 어떠한 상황에서 하는 행동들을 target이라고 볼 수도 있지만, 
여기서는 reinforcement learning방법을 사용해 target은 agent가 경험한 data로부터 정의한다. 

(예 : bootstrapping과 같이 또 다른 prediction을 target으로 사용) 

자세히 알아보기에 앞서 위에서 언급한 value function approximation을 사용하여 parameterized form으로 바꿔 weight에 의한 state value $\hat{v}(S_t,w)$ 계산하는 방법을 알아보도록 하겠다. 

<img src="https://www.dropbox.com/s/ijzf5z0xijraui0/Screenshot%202018-07-11%2019.49.56.png?raw=1">


Weight에 의한 Time step $t$에서의 State $S$는 다음과 같이 계산 가능하다. 




$$\hat{v}(S_t,w) =w^T x = \sum_{i=1}^{d} W_i^T X_i$$


<img src="https://www.dropbox.com/s/ds5o0c4ueil8cff/Screenshot%202018-07-11%2021.18.08.png?raw=1">







# 2.State-action pair value

State value와 마찬가지고 State-action pair를 function approximator를 통해 approximation하는것이 가능하다. 



$$Q(s,a) \approx q_\pi(s,a) \approx \hat{q}(s,a,w)$$




$$\hat{q}(S_t,w) =w^T x(s,a) = \sum_{i=1}^{d} W_i^T X_i(s,a)$$





# 3. Stochastic Gradient Descent (SGD) 

gradient descent는 간단히 현재의 위치에서 기울기에 따라 descent하게 움직이고 여기서는 error을 최소화 하기 위해 $minus(-)$ term으로 인해 descent 한다. 

<img src="https://www.dropbox.com/s/v7xno0sn0fpye6l/Screenshot%202018-07-11%2021.58.47.png?raw=1">


$$w \leftarrow w - \alpha \bigtriangledown_w Error^2_t$$

Reinforcement learning에서는 descent가 아닌 reward 또는 value의 최대를 구하기 위해 $plus(+)$를 사용하여 ascent를 구하며 이것은 추후에 다룰 예정이다.


<img src="https://www.dropbox.com/s/7uvt3l4fikrwsnm/Screenshot%202018-07-11%2021.58.19.png?raw=1">

 


이 SGD를 다음과 같은 형태로 정의할 수 있다.  

##### <Center>General SGD</center>

$$w \leftarrow w - \alpha \bigtriangledown_w Error^2_t$$

 #### <Center>Value Function Approximation</center>
 
$$w \leftarrow w - \alpha \bigtriangledown_w [Target_t - \hat{v}(S_t,w)]^2$$

#### <Center>Chain Rule</center>

$$w \leftarrow w - 2\alpha \bigtriangledown_w [Target_t - \hat{v}(S_t,w)] \bigtriangledown_w [Target_t - \hat{v}(S_t, w)]$$



#### <center>Semi-gradient</center>

$$w \leftarrow w + \alpha \bigtriangledown_w [Target_t - \hat{v}(S_t,w)] \bigtriangledown_w\hat{v}(S_t, w)$$


#### <center>Linear case</center>

$$w \leftarrow w + \alpha \bigtriangledown_w [Target_t - \hat{v}(S_t,w)] x(S_t)$$

#### <center>Action value form</center>

$$w \leftarrow w + \alpha \bigtriangledown_w [Target_t - \hat{q}(S_t,A_t,w)] x(S_t,A_t)$$
 



예를 들어 Monte Carlo Algorithm으로 $\hat{v}$을 approximation 하는 pseudo code는 다음과 같다. 


<img src="https://www.dropbox.com/s/mkq9s8lyn03nkcq/Screenshot%202018-07-13%2004.58.59.png?raw=1">

위의 알고리즘을 Randowalk라는 환경에서 실험해보았고, 아래의 graph와 같이 



<img src="https://www.dropbox.com/s/bor2r51nughkx2u/Screenshot%202018-07-15%2020.59.47.png?raw=1">
---
References


## Reference 
* Reinforcement Learning: An Introduction Richard S. Sutton and Andrew G. Barto Second Edition, in progress
MIT Press, Cambridge, MA, 2017

https://wonseokjung.github.io/



	
