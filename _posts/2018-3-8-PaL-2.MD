---
title: "Planning and Learning with Tabular Methods - 2"
date: 2018-03-08 12:26:28 -0400
categories: Reinforcementlearning update
use_math : true
---
# 6.2 Dyna: Integrating Planning, Acting, and Learning

Planning이 online 일때 환경과 상호작용을 하며 여러가지 이슈가 발생한다. 

환경이 새로운 정보를 받아들이면서 model이 변할 수 있기 때문이다. 

만약 Decision making을 하는 것과 Learning하는 것이 둘다 computational-intensive process라면 computation resources를 분리시켜야 할 필요가 있을 것이다. 

분리시키는 방법으로 여기서 on-line planning agent에 사용할 Dyna-Q learning에 대해 알아보겠다. 

Dyna-Q는 on-line planning agent에서 필요한 아키텍쳐를 통합시키는 메이저함수들이다. 각 함수들은 아주 간단한 형태의 함수이다. 

planning agent에서 real experience의 역할이 두가지이다.
1. 모델을 개선할때 사용 : 진짜 환경에 더욱 잘 맞는 모델을 만들기 위해
2. 강화학습 방법을 사용 : 바로 value function과 policy를 개선하는 것이다. 

1번을 model learning이라고 하며, 2번을 direct reinforcement learning이라고 한다. 


![zxcz](https://user-images.githubusercontent.com/11300712/37135453-be0ed98a-22e0-11e8-89ea-f1d8019f0f5e.JPG)

Learning, planning, acting의 관계는 위의 그림과 같다.

그림의 화살표와 같이 value/policy를 바로 개선하는 방법을 direct reinforcement learning 이라고 하며, model을 거쳐 value/policy를 개선하는 것을 indirect reinforcement learning 하다고 한다. 

Direct method와 Indirect method에는 둘다 장,단점이 있다. 

Indirect method 은 환경과의 상호작용량이 적어도 더 나은 policy를 얻을 있고, Direct method는 간단하며 모델의 영향을 받지 않는다. 

Indirect method가 항상 더 좋다는 주장과, Direct method가 동물과 사람이 배우는 방법이라는 주장이 많다.

하지만 여기서는 두 가지 방법의 다른면을 보지 않고 공통점에 집중하도록 하겠다. 

planning으로 디자인된 DP와 Model-free로 디자인된 TD가 비슷한 점이 있듯이, 

Indirect와 Direct에는 비슷한 점이 존재한다. 

위에서 말한 Dyna-Q는 planning, action, model-learning, direct RL이 포함되며 연속적으로 일어난다. 

여기서planning model은 앞에서 배운  random-sample one-step tabular-Q-planning이고,

(Q-planning)

![zcv](https://user-images.githubusercontent.com/11300712/37020187-4cfb4b80-215e-11e8-8b78-fc1e4c72c895.JPG)

Direct RL method는 one-step tabular Q-learning이다. 

여기서 model-learning method는 table기반이며 환경은 deterministic하다고 가정한다.  

각 transition마다($$S_t , A_t -> R_{t+1},S_{t+1}$$) model이 $$S_t,A_t$$의 했을때 $$R_{t+1},S_{t+1}$$을 기록한다.  

그러므로 모델은 state-action pair을 input으로 받고 prediction을 할때, 

마지막으로 관측되었던 다음 state와 다음 reward를 prediction으로 리턴한다. 

Planning에서 Q-planning Algorithm은 전에 경험하였던 state-action pair만 random하게 sample하기에 ( Select a state, S and an action A, at random) model은 정보가 없는 state-action pair는 절대 queired하지 않는다. 

아래의 구조는 Dyna agent중 하나의 예제로 Dyna-Q algorithm이다.  

![xzcc](https://user-images.githubusercontent.com/11300712/37135452-bde824d4-22e0-11e8-9bed-46ee69a949be.JPG)

agent와 Environment이 상호작용을 하며 real experience를 생성한다. 그 생성된 real experience로 policy와 value function을 업데이트하는데, 바로 업데이트를 하는 direct RL update와 모델을 학습시키는 model learning으로 나뉜다. 


왼쪽의 화살표는 direct RL을 나타낸다. agent와 evironment의 상호작용을 통하여 생성된 real experience를 사용하여
바로 policy/value function을 업데이트한다. 


오른쪽은 Mode-based process이다. 

모델은 real experience를 통하여 배우며, model을 통하여  simulated experience를 생성한다.


여기서 search control이란 단어는 model에 의해 생성된 simulated experience의 시작 state와 action을 선택하는 작업이다. 

그다음, 실제로 발생한 것처럼 강화학습 방법을 simulated experience사용하여 planning update한다.

Dyna-Q에서 강화학습의 방법은 real experience에서의 learning과 simulated experience에서의 planning에 둘다 사용되기 때문에 
강화학습 방법은 planning과 lerning 에서 "final common path"이다.


이론적으로는 Planning, action, model-lerning, directRL이 동시에 병렬적으로 발생하지만, 
Dyna-Q에서는 learning과 planning이 통합되어 시행되며 Tabular Dyna-Q의 pseudo code는 아래와 같다. 



![cxbxcb](https://user-images.githubusercontent.com/11300712/37135451-bdc4c502-22e0-11e8-9659-c4fadf48bb9e.JPG)



$$Model(s,a)$$는 $$state-action pair(s,a)$$의 predict값인 reward와 next state를 의미한다.

step d~f의 의미는,

1. step(d): Direct reinforcement learning,
2. step(e): Model-learning
3. step(f):planning

을 의미한다. 만약 여기서 e와,f가 빠진다면 one-step tabular Q-learning과 동일한 알고리즘이다. 




## Reference 
* Reinforcement Learning: An Introduction Richard S. Sutton and Andrew G. Barto Second Edition, in progress
MIT Press, Cambridge, MA, 2017





