---
title: "DQN을 알아보자"
date: 2018-02-20 10:26:28 -0400
categories: RL_paper update
use_math : true
---




# Playing Atari with Deep Reinforcement Learning






- - -


### Introduction

강화학습에서 고차원의 감각을 (vision과 speach) 같은 것을 input으로 쓰는 것은 오랜 도전이 었습니다. 대부분의 성공한 RL application은 linear value function이나 policy representation을 합친 hand-craft feature에 의지하였습니다. 

최근 딥러닝에서 raw sensory data ( vision, speach ) 를 추출하는 것이 가능해졌습니다. 


딥러닝을 강화학습에 적용하기 위해 연구가 진행되었고, 딥러닝을 적용하기에 몇가지 어려운 점을 발견하였습니다 .

1. 거의 대부분의 딥러닝 어플리케이션은 손으로 labeled된 많은 량의 training data가 필요합니다. 반면에 RL은 spare, noisy, delayed 한 reward라는 scalar값의 신호에서 배워야 합니다. 

2. 두번째 이슈는 대부분의 딥러닝 알고리즘은 datar가 독립적인 반면, reinforcement는 sequence가 연관성이 높습니다. 

3. RL 에서는 게다가 알고리즘이 새로운  behavior을 배울때마다 data의 distribution이 변한다는 것입니다. 


이 논문은 convolutional neural network가 이러한 문제점을 해결할 수 있다는 것에 대한 논문입니다. 



### Background

Agent가 환경 $$E$$와 상호작용을 하는 task를 보겠습니다.

action, observations, reward의 sequence에서 각 time step 마다 agent는 action at를 선택합니다. 

이 action은 game action의 집합 $$A={1,......,l}$$에서 가져온 것입니다. 

Action이 Emulator을 통과하여 state를 변경하고 score을 받아옵니다. agent는 emulator의 $$x_t \in \mathbb{R}^d$$ 의 이미지를 관찰합니다. 

이 image data $$x_t$$ 는 raw pixel의 vector로 현재 screen을 표시합니다.  

각 time-step 마다 agent가 action $$a_t$$를 선택하면 Emulator을 통해 state를 수정하고 reward $$r_t$$가 return 됩니다. 

우리가 실제로 현재 상황을 파악할때는 현재의 정보 이외에 이전의 정보도 필요한데요. 

여깃 Agent는 input data로 현재 screen에 있는 현재의 image로 받기 때문에 Agent가 한 장면의 화면만 볼수 있다면, 전체의 상황을 전부 이해하기는 힘들 것입니다. 

그래서 action을 했을때의 sequence를 관찰하고,$${x_1,a_1,x_2,a_2,... a_{t-1},x_T}$$ 이 sequence를 사용하여 게임전략을 학습합니다. 


{\color{Blue}아래의 식과 같이 Agent의 목표는 action을 하며 환경과 상호작용을 하면서 return된 미래의 reward를 최대화 시키는 것입니다.}

$$Q^*(s,a)= \underset{\pi}{max}[R_t \mid s_t=s, a_t=a, \pi]$$


많은 강화학습의 기본적인 아이디어는 iteration update되는 bellman equation을 이용하여 action-value function을 측정하는 것입니다. 



$$Q_{i+1}(s,a)=E_{s'\sim  \epsilon } [r+ \gamma \underset{a'}{max} Q^*(s,a)\mid s_t=s, a_t=a]$$

value iteration에서는 i가 infinity가 됨에따라 $$Q_i -> Q*$$로 converge된다고 배웠습니다. 하지만 이 방법은 실제로 비현실적 입니다. 

왜냐하면 action-value function은 각 sequence마다 독립적으로 측정되기 때문입니다. 

대신 function approximator을 사용하여 action-value function을 estimate하는 방법도 있습니다. 

$$Q(s,a ;\theta) \approx Q^*(s,a)$$ 

여기서 neural network function approximator을 위와같이 weight theta를 Q-network로 사용합니다. 

Q-network는 각 iteration i 의 loss function $$L_i(\theta_i)$$를 minimize하기 위해 train합니다. 

$$L_i(\theta_i) = E_{s,a\sim p(.) [(y_i - Q(s,a : \theta_i))^2]}$$



$$y_i$$는 $$i$$에서의 target값 입니다. $$p(s,a)$$는 sequence s와 a의 확률분포이고 behavior distribution을 뜻합니다. 
supervised learning에서의 은 고정되있는 반면, 여기서의  Target은 network의 weight에 따라 변합니다. 


추가로 여기서는 Offpolicy사용합니다. 
behavior distribution을 따라 state들을 exploration 하면서 greedy strategy $$a= max Q(s,a;theta)$$를 선택하는 것입니다. 

실제로 이 behavior distibution은 %%\epsilon-greedy$$입니다.

### 4. Deep Reinforcement Learning

딥러닝을 강화학습에 연결하여 RGB data를 input으로 받는것이 가능하였고, 

연속된 데이터의 상관관계를 없애기 위하여 Experience replay사용해서 각 time step의 agent의 경험을 저장하였습니다. 
$$E_t=(s_t,a_t,r_t,s_{t+1})$$ data set에 $$D={e_1,e_2,e_3...e_n}$$로 저장합니다.$$

experience memory에 저장을 한 뒤 store된 sample을 random으로 뽑아서 q-learning에 적용하였습니다

agent는 e-greedy action을 선택하였고,

고정된 길이로 input의 history를 만들어 input으로 사용했습니다. 이것을 $$\phi$$모양이라고 표현했습니다. 


pseudocode는 아래와 같습니다. 


![default](https://user-images.githubusercontent.com/11300712/36404816-74475f1e-1630-11e8-906f-d4a7c1ef667a.JPG)


이 DQN은 온라인 기본적인 q-learning에 비해 여러가지 장점이 있습니다. 
1. sample들 끼리의 correlation때문에 연이어진 data에서 배우는 것은 비효율적입니다. 그렇기 떄문에 random하게 뽑아서 correlation을 깨고 variance를 낮출수 있다. 

2. experience replay를 통하여 배울때는 off-policy를 하여야 합니다. 왜냐하면 현재의 parameter이 sample생성하는 것고 다르기 때문입니다. 

### Preprocessing and Model Architecture

210 * 160 pixel image를 사용하였고 128개의 색이 있는 이미지를 그레이로 바꾸고 이미지 크기를 84 * 84 로 바꿨습니다.

여기서 파이는 4장의 frame 들의 history를 stack하여 Q-function  input으로 사용하였습니다. 

84 *84*4 의 히스토리를 input으로 하고 convolution layer과 output layer full connected layer 을 통과하여  

 action을 return합니다. 


 
 ![zxczc](https://user-images.githubusercontent.com/11300712/36405081-f4925e70-1631-11e8-96e2-5afa2680b222.JPG)

위와 같이 게임 중 일부는 게임 플레이를 사람과 비슷하게 또는 더 잘하였습니다. 

이 연구에서 파생된 연구들이 많아 저의 강화학습 첫번째 논문으로 DQN을 읽어보았습니다. 

앞으로 DQN에서 파생된 논문들, 강화학습관련 논문들을 주로 다룰 생각힙니다.


## Reference 
* Playing Atari with Deep Reinforcement Learning
https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf






































