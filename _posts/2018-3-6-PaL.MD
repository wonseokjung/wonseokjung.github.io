---
title: "Planning and Learning with Tabular Methods - 1"
date: 2018-03-06 12:26:28 -0400
categories: Reinforcementlearning update
use_math : true
---
# 6. Planning and Learning ith Tabular Methods

이번에는 환경의 모델이 필요한 방법과 필요하지 않은 강화학습 방법을 통합적인 관점으로 보도록 하겠습니다. 

환경의 모델이 필요한 방법 : Dynamic programming, Huristic search

환경이 모델이 필요없는 방법 : Monte Carlo, temporal difference

것들을 각각 Model based와 Model Free라고 부른다. 

Model based는 planning을, Model free는 learning을 한다. 

위의 두가지 방법은 value funtion을 측정한다는 공통점이 있다. 미래의 상황을 예측하고 value를 측정해 이 value를 update target으로 사용하여

value function을 approximate한다.이 공통점을 이용하여, 이번 챕터에서는 model based과 model free learning을 통합 하겠다. 

환경에 model은 agent predict하기위해 사용하는 것이다.



## 6.1 Models and Planning

모델은 state와 action을 받으면 그에 맞는 다음 state와 reward를 return한다.

만약 model이 stochastic한 경우, return된 state 혹은 reward가 여러개이다. 

Model의 종류로는, 

1. distribution models : 모든 가능성과 확률을 산출하는 모델( Dynamic programming)
2. Sample models : 어떠한 한 확률에 의해 뽑힌 sample만을 생성하는 모델 ( Monte carlo method )

Model은 simulation에 사용된다. 시작 State와 action이 주어진 경우, 

1. sample model은 하나의 가능한 transition을 생성한다.

2. distribution model 은 일어날 확률 $$P(S',r \mid s,a )$$ 에 의해 weighted되어진 가능성 있는 모든 transition을 생성한다. 






(Sample Model의 예)

<img width="604" alt="mc" src="https://user-images.githubusercontent.com/11300712/37016032-a480fbfe-214d-11e8-9767-60435bbe8e72.png">


(Distribution model 의 예)

![dp](https://user-images.githubusercontent.com/11300712/37016033-a4b6b5a0-214d-11e8-81b3-326c9e97b36a.jpg)



State와 Policy가 주어지는 경우,

1. Sample model은 하나의 전체 Episode을 생성한다. 

2. Distribution model은 가능한 모든 episode와 그것들의 확률을 생성한다. 

우리는 Model을 환경을 simuate하기 위해서, simulated experience를 생성하기 위해 사용한다. 


강화학습에서 planning의 의미는,

input을 model 받고 policy를 생성 또는 개선하는 계산프로세스을 의미한다.


$$ model\overset{planning}{\rightarrow} policy$$

인공지능에서 planning의 2가지 접근법이 있다. 첫번째는 state space planning이고 두번째는 plan-space planning이다.

우리는 state space를 통해 optimal policy를 찾는 state space planning에 대해서 다루겠다. 


state-space planning는 2가지 기본 아이디어가 있다. 


1. 모든 state-space planning방법은 policy를 개선하기 위해 value function을 계산한다. 
2. value function을 update 또는 backup 을 통해 계산한다. 

이 구조는 다음과 같이 나타낼 수 있다. 


$$ model \rightarrow simulated experience \overset{backups}{\rightarrow} values \rightarrow policy $$

우리가 전에 배웠던 learning과 planning의 같은점은 둘다 Back-up을 통하여 value function을 estimate하는 것이다. 

다른점은 planning는 model에 의해 생성된 simulated experience를 쓴다는 것이고, 

learning은 환경에 의해 생성된 real experience를 사용하는 것이다. 

많은 경우에 learning algorithm이  planning method의 update 방법으로 대체될 수 있다. 

밑의 박스는 planning method를 기반으로 한 one-step tabular Q-learning의 예이다.

sample model에서 임의로 sample한 one-step tabular Q-learning의 예제다. 


이 방법을 random-sample one-step tabular Q-planning라고 한다.


![zcv](https://user-images.githubusercontent.com/11300712/37020187-4cfb4b80-215e-11e8-8b78-fc1e4c72c895.JPG)


## Reference 
* Reinforcement Learning: An Introduction Richard S. Sutton and Andrew G. Barto Second Edition, in progress
MIT Press, Cambridge, MA, 2017

* https://goo.gl/images/vKKXU7





