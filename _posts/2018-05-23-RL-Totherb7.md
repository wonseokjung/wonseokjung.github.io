---
title: "ì œ3í¸: To the Rainbow"
date: 2018-5-23 12:26:28 -0400
categories: Reinforcementlearning update
use_math : true
---

---
<!-- $theme: default -->

# To the Rainbow ğŸŒˆ




<center>Wonseok Jung</center>


---



<center><img src="https://user-images.githubusercontent.com/11300712/40341700-7370828a-5d3b-11e8-84c4-39165da1b014.png"></center>


---
# 1. Abstract 

Six extensions to the DQN algorithm and empirically studies their combination

---

# 2.Introduction

<center><img src="https://user-images.githubusercontent.com/11300712/40341998-1beb093e-5d3d-11e8-90e0-c065b17af4a8.png" weight="400" height="400" ></center> 

ê°•í™”í•™ìŠµì—ì„œ ìµœê·¼ ë³µì¡í•œ sequential decision-making problemsì„ í•´ê²°í•œ ê²ƒì€ Deep Q-Networks algorithmsìœ¼ë¡œ ë¶€í„° ì‹œì‘í•˜ì˜€ë‹¤. 


---

# 2.2 Performance 

<center><img src=" https://user-images.githubusercontent.com/11300712/40342389-41baee8e-5d3f-11e8-8fc8-d6f9820e2109.png"width="420" height="420"></center>
  
  
57ê°œì˜ Atari gameì—ì„œ DQNì˜ ì„±ëŠ¥ë¹„êµë¥¼ í•´ë³¸ ê²°ê³¼, Rainbow DQNì˜ performanceê°€ ê°€ì¥ ì¢‹ì•˜ë‹¤. 

 




---



# 3. Background


- ê°•í™”í•™ìŠµì—ì„œ agentëŠ” rewardë¥¼ maximizeí•˜ê¸°ìœ„í•œ actionì„ ì„ íƒí•œë‹¤. 
- ê° time step  $t=0,1,2,...$ ë§ˆë‹¤ environemtëŠ” state $S_t$ë¥¼ ì œê³µí•œë‹¤. 
- AgentëŠ” action $A_t$ë¥¼ ì„ íƒí•œë‹¤. 
- EnvironmentëŠ” $A_t$ ë°›ê³  Reward $R_{t+1}$, ë‹¤ìŒ state $S_{t+1}$ì„ ì œê³µí•œë‹¤. 


---

# 3.1 Agent and Evironment

- ì´ëŸ° interactionì„ Markov Decision Process í˜¹ì€ MDPë¼ê³  í•œë‹¤. 
- ì´ MDPëŠ” $<S, A,T,R, \gamma>$ ì˜ Tupleì´ë‹¤. 
- S : finitie set of states
- A : finite set of actions
- $T(s,a,s')$ : transition function , $P[S_{t+1}=s' \mid S_t =s, A_t =a]$
- $r(s,a)$  : reward function, $E[R_{t+1} \mid S_t = s, A_t =a ]$ 
- $\gamma \in [0,1]$ : discount factor 


---




# 3.1 Agent and Environment


- Policy $\pi$ : probability distribution over actions for each States
- AgentëŠ” ì£¼ì–´ì§„ polcy $\pi$ë¥¼ ë”°ë¼ actionì„ ì„ íƒí•œë‹¤. 
- AgentëŠ” actionì„ ì„ íƒí•˜ë©° Stateë§ˆë‹¤ rewardë¥¼ collect í•œë‹¤. 
 $$ G_t = \sum^{\infty}_{k=0} \gamma_t^{(k)} R_{t+k+1} $$
- AgentëŠ” ìœ„ì˜ discounted returnì„ ìµœëŒ€í™” í•˜ê¸° ìœ„í•œ policyë¥¼ ì°¾ëŠ”ë‹¤. 





---
# 3.1 Value-based reinforcement learning


- Agent learns an estimate of the expected discounted return or value
- Given state : $v^\pi (s) = E_{\pi}[G_t \mid S_t = s]$
- Given state-action pair : $q^\pi (s,a) = E_{\pi}[G_t \mid S_t =s, A_t = a]$
	
    
---
# 3.1 Obtain new policy from state-action value function
- action valuesì— ëŒ€í•˜ì—¬ $\epsilon-greedily$ í•˜ê²Œ actionì„ ì„ íƒí•œë‹¤.
- $greedy$ action : ê°€ì¥ ë†’ì€ valueë¥¼ ë°›ëŠ” actionì„ ì„ íƒ , $(1-\epsilon)$
- non $greedy$ action : $\epsilon$ì˜ í™•ë¥ ë¡œ actionì„ randomí•˜ê²Œ ì„ íƒí•œë‹¤. 
- ìœ„ì˜ policyë¡œ ì¸í•´ agentëŠ” explortionì„ í•˜ë©° ìƒˆë¡œìš´ policyë¥¼ ì°¾ëŠ”ë‹¤. 


---

# 3.2 Deep reinforcement learning and DQN

- DQN : deep networkì™€ reinforcement learningì´ ê²°í•©í•œ ì•Œê³ ë¦¬ì¦˜
- CNNì„ ì´ìš©í•˜ì—¬ state $S_t$ì˜ action valueë¥¼ approximation
- AgentëŠ”   $\epsilon-greedily$ í•˜ê²Œ actionì„ ì„ íƒ
- Transition $(S_t, A_t, R_{t+1}, \gamma_{t+1}, S_{t+1})$ì„ replay memory bufferì— ì¶”ê°€
---
 
 # 3.2 Deep reinforcement learning and DQN
 
- Lossë¥¼ minimizeí•œë‹¤. 
	- $(R_{t+1} + \gamma_{t+1} max_{a'} q_{\overline{\theta}}(S_{t+1}, a^{'}) - q_{\theta}(S_t,A_t))^2$
	- time step $t$ëŠ” replay memoryì—ì„œ randomly picked
	- ${\theta}$ : Online networkì˜ parameter(Back-propagated only into parameter ${\theta}$) 

	- ${\overline{\theta}}$ : Target Networkì˜ parameter( Periodic copy of the online network)
	



---
# 4. Extension to DQN 

- Double DQN
- Prioritized experience replay
- Dueling network architecture
- Multi-step bootrap targets
- Disributional Q-learning
- Noisy DQN


---

# 4.1 Double DQN 
- Q-learningì€ maximizationí•˜ëŠ” ë¶€ë¶„ë•Œë¬¸ì— overestimation ë¬¸ì œ ë°œìƒ
- ì´ë¡œ ì¸í•´ Learning íš¨ìœ¨ì´ ë–¨ì–´ì§„ë‹¤. 
- Double Q-learning : Targetê³¼ evaluationì„ ë¶„ë¦¬ 
	1. Target : maximization performed for the bootstrap
	2. selection of the action from its evaluation
- DQNê³¼ combine 
$(R_{t+1} + \gamma_{t+1} q_{\overline{\theta}}(S_{t+1}, argmax_{a'}q(St) - q_{\theta}(S_t,A_t))^2$

---


# 4.2 Prioritized replay

- DQNì€ replay bufferì—ì„œ uniformí•˜ê²Œ samplingí•˜ì—¬ í•™ìŠµí•œë‹¤. 
- í•™ìŠµì´ ë” í•„ìš”í•œ transitionsì„ sample í•˜ëŠ” ê²ƒì´ ë” ì´ìƒì ì¸ ë°©ë²•ì¼ ê²ƒì´ë‹¤. 
- Prioritized experience replay samples transitions with probability $p_t$
 relative to the last encountered absolute TD error
 
 
$$p_t  \propto \mid R_{t+1} + \gamma_{t+1} max_{a^{'}} q_{\overline{\theta}}(S_{t+1}, a^{'}) - q_{theta}(S_t,A_t) \mid^{\omega}$$

---
 
# 4.3 Dueling networks
 

- Dueling networkëŠ” ë‘ computationìœ¼ë¡œ ì¤„ê¸°ë¥¼ ë‚˜ëˆˆë‹¤ : Value , Advanage

$$q_{\theta}(s,a) = v_{\eta}(f_{\xi}(s)) + a_{\psi}(f_{\xi},a) -  \frac{\sum_{a^{'}} a_{\psi}(f_{\xi}(s)),a^{'})}{a}$$

- ${\xi}$, ${\eta}$, ${\psi}$ : encoder $f_{\xi}$, value $v_{\eta}$, advantage ì˜ share parameter
- $\theta={ ({\xi},{\eta},{\psi}) }$ : concatenation


![screen shot 2018-05-22 at 16 54 43](https://user-images.githubusercontent.com/11300712/40396354-6ccd34e0-5de1-11e8-94c4-3a8bc809510f.png)

---

# 4.4 Multi-step learning

- Q-learningì€ í•˜ë‚˜ì˜ rewardë¥¼ ë°›ê³ , bootstrapì„ ìœ„í•´ ë‹¤ìŒ stepì—ì„œ greedy actionì„ í•œë‹¤.
- ëŒ€ì‹ , multi-stepì„ ì‚¬ìš© í•˜ì—¬ n-step ë‹¤ìŒì„ bootstrapí•  ìˆ˜ ìˆë‹¤. 


$$R_t^{(n)}\equiv \sum^{n-1}_{k=0} \gamma_{t}^{(k)} R_{t+k+1}$$

---

# 4.4 Multi-step learning

- DQNì—ì„œ Multi-stepì˜ LossëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤. 

$$(R_{t}^{(n)} + \gamma_{t}^{(n)} max_{a^{'}}q_{\overline{\theta}}(S_{t+n},a^{'}) - q_{\theta}(S_{t},A_{t}))^{2}$$

- Multi-step targets with suitably tuned n often lead to faster learning(Sutton and Barto 1988)

---

# 4.5 Distributional RL - Idea (Monopoly game)


- ìƒëŒ€ë°©ì´ ì†Œìœ í•œ ë•…ì— ë„ì°© : $ 2000ì˜ penalty

- ë¬´ì‚¬íˆ í†µê³¼í•˜ì˜€ì„ë•Œ : $200 ì˜ reward 
- Reinforcement learnig ê´€ì ì—ì„œ Expect rewardë¥¼ ê³„ì‚°

$$ER(x) = \frac{35}{36} \times 200 - \frac{1}{36} \times 2000 \approx 139$$

---

# 4.5 Idea - Monopoly

- Reinforcement learningì—ì„œëŠ” ëŒ€ë¶€ë¶„ ìœ„ì˜ ì˜ˆë³´ë‹¤ ë” ë³µì¡í•˜ë‹¤. 

$$R(x_0) + \gamma R(x_1) + \gamma^{2} R(x_2)  + \gamma^{3} R(x_3) ...$$

- Sum of discounted reward

$$V^{\pi}(x) = E_{p^{\pi}} [ \sum^{\infty}_{t=0} \gamma^{t}R(x_t) \mid x_0 =x]$$

$$= E R(x) + \gamma E_{x^{'} ~P^{\pi}}V^{\pi}(x^{'})$$


---

# 4.5 Idea 

- next stateì˜ actionì„ bootstrap + r ê³¼ current state ì˜ valueë¥¼ minize ì‹œí‚¤ëŠ” ê²ƒì´ ì•„ë‹Œ

- ë‹¤ìŒ step ì˜ distributionê³¼ current stateì˜ distributionì„ minimize í•˜ì
-  Bellman equation 

$$Q(s,a) = ER(s,a,s^{'}) + \gamma EQ(s',a')$$

- Distributional Bellman equation
$$Z(s,a) = R(s,a,S') + \gamma Z(S',A')$$

---
# 4.5 New Algorithm - C51
- next stateì˜ ëª¨ë“  distributionì„ ê°€ì ¸ì˜¨ë‹¤.
- ê·¸ distributionì„ backup í•˜ì—¬ current guess ë¡œ ì‚¬ìš©í•œë‹¤. 
---
# 4.5 The C51 Algorithm

- C51ì€ ê° Q-valueì˜ softmax ë¥¼ returní•œë‹¤.
- Wasserstein distance ë¥¼ minimizeí•œë‹¤. 
(ì°¸ê³ : https://mtomassoli.github.io/2017/12/08/distributional_rl/)

 <center><img src="https://user-images.githubusercontent.com/11300712/40403652-055fbb1c-5e08-11e8-8185-56a4247b3449.png" weight = "300" height = "300"></center>



---

# 4.6 Noisy Nets
- $\epsilon-greedy$ì˜ exploration ë°©ë²•ì€ íŠ¹ì • í™˜ê²½ì—ì„œ í•œê³„ë¥¼ ë³´ì¸ë‹¤. 
- Noisy Netsì„ ì‚¬ìš©í•˜ì 
- ì°¸ê³  : https://wonseokjung.github.io//reinforcementlearning/update/RL-Totherb-2/
 
$$y = (b+Wx) + (b_{noisy} \odot \epsilon^{b} + (W_{noisy} \odot \epsilon^{w} )x)$$

---

# 4.6 Noisy Nets

 
$$y = (b+Wx) + (b_{noisy} \odot \epsilon^{b} + (W_{noisy} \odot \epsilon^{w} )x)$$

- $\epsilon^{b},\epsilon^{w}$ : random variable 
- $\odot$ : element-wise product 

---
# 5. Rainbow 

- Rainbow DQNì€ ìœ„ì˜ ì–¸ê¸‰ëœ six extenstion DQNì´ ëª¨ë‘ ì ìš©ëœ ë²„ì „ì´ë‹¤.ğŸ˜‚  
- ê¸°ì¡´ DQNì— ë¹„í•´ ì›”ë“±í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, muti-step ë˜ëŠ” priorityë¥¼ ì œì™¸í•˜ì˜€ì„ë•Œ ë ˆì¸ë³´ìš°ì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì¡Œë‹¤. 

<center><img src="https://user-images.githubusercontent.com/11300712/40406098-2d98fcb8-5e14-11e8-83d3-a79a72e52e73.png"></center>





---
# 5. Rainbow -Atariì„±ëŠ¥ ë¹„êµ

-ì•„íƒ€ë¦¬ 57ê°œì˜ í™˜ê²½ì—ì„œ ë¹„êµí•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. 
<center><img src="https://user-images.githubusercontent.com/11300712/40406097-2d68ee60-5e14-11e8-94c5-39af361fc2ab.png"></center>

---


# 5. Rainbow Hyper-parameters


- Six extenstion DQNì´ ëª¨ë‘ ì ìš©ë˜ì—ˆìœ¼ë©°,  

- Rainbow DQN ë…¼ë¬¸ì—ì„œ ì ìš©ëœ hyper parameterëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

<center><img src="https://user-images.githubusercontent.com/11300712/40405729-a1cd9f5a-5e12-11e8-9417-0b61460cca6a.png" weight="350" height="350"></center>

---

References 

Dueling Network Architectures for Deep Reinforcement Learning
https://arxiv.org/pdf/1511.06581.pdf

Rainbow: Combining Improvements in Deep Reinforcement Learning

https://arxiv.org/pdf/1710.02298.pdf

https://wonseokjung.github.io

---

























