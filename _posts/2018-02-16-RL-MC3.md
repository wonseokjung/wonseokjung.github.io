---
title: "Monte Carlo Method-3"
date: 2018-02-16 22:26:28 -0400
categories: Reinforcementlearning update
use_math : true
---


# 3.4 Monte Carlo Control without Exploring Starts 

이전 chapter에서 Monte Carlo method는 경험을 통해 얻은 샘플로 value를 측정한다고 하였습니다. 

모델이 없기 때문에 state action value를 측정하는 것이 더 효과적이라고 하였고, 최대한 다양한 state를 탐험하게 하려고 exploring start를 사용한다고 하였습니다. 

Exploring start를 사용하지 않고 위의 효과를 낼수 있는 방법은 무엇이 있을까요?

가장 general한 방법으로는 agent가 모든 action을 전부 선택하는것 입니다.
그렇게 하기 위해서On-policy와 Off-policy의 두가지 방법이 있는데요, 

_ _ _

On-policy : 의사결정을 하기위해 Policy를 Evaluate하거나 improve하는것을 말합니다. 
Off policy : policy different를 evalue하거나 improve합니다. 

_ _ _

***먼저 on-policy 부터 보도록 하겠습니다.***

On-policy에서 policy는 soft합니다. Soft한다는 것의 의미는 모든 state s, action a 에서 
$$\pi(s\mid a) > 0$$ 이고 점점 deterministic한 optimal policy로 다가갑니다. 
여기서 On-policy 방법으로 $$\varepsilon-greedy$$ 를 사용합니다. 

Epsilon greedy는 대부분 action value가 가장 큰 것을 사용하지만 $$\varepsilon$$의 확률action을 임의로 선택합니다.

On-policy Monte Carlo Control의 전체적인 Idea또한 GPI 입니다. Monte Carlo CS에서는 first-visit을 이용하여 현재 policy에대한 action-value를 측정합니다. 

여기서의 On-policy의 방법으로 $$\varepsilon-greedy$$를 사용한 것입니다. 

$$\varepsilon-greedy$$ 에 대해 생성된 $$\pi$$를 따른 $$q_\pi$$는 $$\pi$$보다 같거나 더 좋을것은	 gurantee합니다. 

On-policy를 이용한 MC 컨트롤의 수도코드는 다음과 같습니다. 



![screenshot from 2018-02-16 22-39-40](https://user-images.githubusercontent.com/11300712/36310125-61e9c3de-136a-11e8-89b7-5b09b7de803d.png)


오늘은 온폴리시 방법를 몬테카를로 컨트롤에 적용해봤습니다. 다음 포스팅은 온폴리시방법이 아닌 오프폴리시를 몬테카를로에 적용하는 방법을 알아보도록 하겠습니다.



## Reference 
* Reinforcement Learning: An Introduction Richard S. Sutton and Andrew G. Barto Second Edition, in progress
MIT Press, Cambridge, MA, 2017
































