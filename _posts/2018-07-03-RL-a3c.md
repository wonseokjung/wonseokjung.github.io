---
title: "A3C - Asynchorous Advantage Actor Critic Network"
date: 2018-07-03 12:26:28 -0400
categories: Reinforcementlearning update
use_math : true
---



# The Asynchronous Advantage Actor Critic Network - A3C 

wonseok Jung

---

![screen shot 2018-07-02 at 16 49 19](https://user-images.githubusercontent.com/11300712/42191485-f51ead46-7e17-11e8-9e45-8eb869c834ac.png)


1. DQN은 많은 computation을 요구한다. 
2. Google deepind에서 computation 이슈를 해결하기 위한 알고리즘을 만들어냄 

---
# 1. A3C 

![a3c 003](https://user-images.githubusercontent.com/11300712/42199406-33a3841c-7e43-11e8-8163-544907876e83.jpeg?style=centerme)

1. 그 알고리즘을 Asynchronous Advantage Actor Critic Network ( A3C ) 라고 정의함
2. A3C는 DQN보다 적은 computation을 요구하며 training 시간 또한 짧다. 



---
## 1.1 A3C의 학습방법, 적용 action spaces

![a3c 004](https://user-images.githubusercontent.com/11300712/42199407-33d1a040-7e43-11e8-9b88-dd9379d3c119.jpeg)

1. A3C의 idea는 agent를 여러개 만들어 병렬로 학습하는 방법이다. 
2. A3C는 continuous action 또는 discrete action spaces에서 둘다 적용할 수 있다. 


---

## 1.2 A3C의 특징인 global network 

![a3c 005](https://user-images.githubusercontent.com/11300712/42199408-33fd920e-7e43-11e8-9a41-cd6a2802c476.jpeg)

1. 여러 agents ( 혹은 workers )가 각 environment에서 pareller하게 학습한다. 

2. 각 agent가 수집한 experience는 global agent로 aggregated 된다. 

3. Global agent는 master network 혹은 global network라고 불린다.

---
# 2. A3c에서 AAA의 의미는 무엇일까 ? 


## 2.1 A3C에서 AAA 의 의미 - Asynchronous? 
![a3c 006](https://user-images.githubusercontent.com/11300712/42199409-342a8f20-7e43-11e8-8a62-1d82ac777c01.jpeg)

1. Asynchronous : 
- DQN은  하나의 Agent가 Environment와 interactrion하며 optimal policy를 찾기위해 학습을 한다. 
- A3C는 multiple agents가 각 environment와 interaction하며 학습한다. 


## 2.2 A3C에서 AAA 의 의미 - Advantage ?  
![a3c 007](https://user-images.githubusercontent.com/11300712/42199410-3456b8ac-7e43-11e8-9945-626390cf1235.jpeg)
1. Advantage : 

- Advantage function은 Q function과 value function의 차이이다. 
- Q function은 action value가 얼마나 좋은지 측정하는 것이고, value function은 state value가 얼마나 좋은지 측정하는 것이다. 
- 이 둘의 차이를 직관적으로 비교해보면, agent가 action을 하는 것이 모든 action들보다 얼마나 나쁜지 혹은 좋은지 측정하는 것이다. 


---


## 2.3 A3C에서 AAA 의 의미 - Actor - ?  

![a3c 008](https://user-images.githubusercontent.com/11300712/42199411-347fa55a-7e43-11e8-9877-7fa4fb24ffbc.jpeg)

1. Actor-critic 
- Architecture는 두가지 network로 Actor와 Critic이 있다. 
- Actor : policy를 배운다. 
- Critic : Actor에 의해 배운 policy가 얼마나 좋은지 Critic한다. 

---

# 3.Architecture of A3C 


![a3c 009](https://user-images.githubusercontent.com/11300712/42199412-34b066b8-7e43-11e8-9df0-690e1b9a99a1.jpeg)


- multiple agents가 각 환경에서 interaction을 한다. 
- 각 agent는 policy를 배우고, policy loss의 gradient를 계산하여 gradient를 global network로 update한다. 
- A3C는 여러 Agent가 환경과 interaction하며 global network로 aggregate하며 experience의 correlate를 없앤다. 
- 그렇기 때문에 replaymemory를 사용하지 않기 때문에 storage와 computation time이 줄어든다. 
